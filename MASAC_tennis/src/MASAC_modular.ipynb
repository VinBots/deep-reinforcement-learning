{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Soft Actor Critic Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements the Soft Actor-Critic Algorithm as documented in the paper [here](https://arxiv.org/pdf/1812.05905.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Soft_Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_size):\n",
    "        super(Soft_Q_Network, self).__init__()\n",
    "\n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.fc3 = nn.Linear(self.h2_size, self.output_size)\n",
    "\n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_w,init_w)\n",
    "\n",
    "    def forward(self, state,action):\n",
    "        x = torch.cat([state,action],1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Soft_Policy_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_mean_size, output_std_size):\n",
    "        super(Soft_Policy_Network, self).__init__()\n",
    "\n",
    "        # state, hidden layer, action sizes\n",
    "        self.input_size = input_size\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.output_mean_size = output_mean_size\n",
    "        self.output_std_size = output_std_size\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.h1_size)\n",
    "        self.fc2 = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.fc3_mean = nn.Linear(self.h2_size, self.output_mean_size)\n",
    "        self.fc3_log_std = nn.Linear(self.h2_size, self.output_std_size)\n",
    "        #initialize weights\n",
    "        init_w = 3e-3\n",
    "        self.fc3_mean.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_mean.bias.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.weight.data.uniform_(-init_w,init_w)\n",
    "        self.fc3_log_std.bias.data.uniform_(-init_w,init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc3_mean(x) #values of the action should be between -1 and 1 so this is not the mean of the action value\n",
    "        log_std = self.fc3_log_std(x)\n",
    "        log_std_min = -20\n",
    "        log_std_max = 0\n",
    "        log_std = torch.clamp(log_std,log_std_min, log_std_max)\n",
    "        return mean,log_std\n",
    "\n",
    "    def sample (self,state,epsilon = 1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal (mean,std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        log_pi = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_pi = log_pi.sum(1,keepdim=True)\n",
    "        return action, log_pi\n",
    "\n",
    "    def get_action(self, state,deterministic):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        mean, log_std = self.forward(state)\n",
    "\n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            std = log_std.exp()\n",
    "            normal = Normal(mean, std)\n",
    "            z = normal.sample() #sample an action from a normal distribution with (mean,std)\n",
    "            action = torch.tanh(z) #squeeze the value between -1 and 1\n",
    "\n",
    "        action = action.cpu().detach().squeeze(0).numpy()\n",
    "        return self.rescale_action(action)\n",
    "\n",
    "    def rescale_action(self, action):\n",
    "        action_range=[-1,1]\n",
    "        return action * (action_range[1] - action_range[0]) / 2.0 +\\\n",
    "            (action_range[1] + action_range[0]) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centralized_ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed,num_agents):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.num_agents = num_agents\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.all_fields_names = [\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"]\n",
    "        self.record_length = len(self.all_fields_names)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=self.all_fields_names)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        ====\n",
    "        Data Structure.\n",
    "\n",
    "            Each experience is made of a list of states (s), a list of actions (a),\n",
    "            a list of rewards (r), a list of next_states(s') and a list of dones (d)\n",
    "\n",
    "            The i-th element of each list of s,a,r,s' records the s,a,r,s for agent of index i\n",
    "            For example, experience of agent2 =\n",
    "            (experience.states[2],experience.actions[2],experience.rewards[2],experience.next_states[2]\n",
    "        \"\"\"\n",
    "\n",
    "    def add(self, states, actions, rewards, next_states, dones):\n",
    "\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e=self.experience._make((states, actions, rewards, next_states, dones))\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \"\"\"Returns torch tensors\"\"\"\n",
    "\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        all_states = tuple(torch.from_numpy(np.vstack([e.states[i] for e in experiences if e is not None]))\\\n",
    "                                   .float().to(self.device) for i in range (self.num_agents))\n",
    "\n",
    "        all_actions = tuple(torch.from_numpy(np.vstack([e.actions[i] for e in experiences if e is not None]))\\\n",
    "                                   .float().to(self.device) for i in range (self.num_agents))\n",
    "\n",
    "        all_rewards = tuple(torch.from_numpy(np.vstack([e.rewards[i] for e in experiences if e is not None]))\\\n",
    "                                   .float().to(self.device) for i in range (self.num_agents))\n",
    "\n",
    "        all_next_states = tuple(torch.from_numpy(np.vstack([e.next_states[i] for e in experiences if e is not None]))\\\n",
    "                                   .float().to(self.device) for i in range (self.num_agents))\n",
    "\n",
    "        all_dones = tuple(torch.from_numpy(np.vstack([e.dones[i] for e in experiences if e is not None]).astype(np.uint8))\\\n",
    "                          .float().to(self.device) for i in range(self.num_agents))\n",
    "\n",
    "        return (all_states, all_actions, all_rewards, all_next_states, all_dones)\n",
    "\n",
    "    def buffer_len(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Agent:\n",
    "\n",
    "    def __init__(self, index,state_dim = 24 ,action_dim = 2,layer_size = 128, qf_lr = 0.0006, \\\n",
    "                 policy_lr=0.0003,a_lr = 0.0006,auto_entropy_tuning=True,soft_target_tau =0.02, discount = 0.99):\n",
    "\n",
    "        self.index= index #starts at 0\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.layer_size = layer_size\n",
    "        self.qf_lr = qf_lr\n",
    "        self.policy_lr = policy_lr\n",
    "        self.a_lr = a_lr\n",
    "        self.auto_entropy_tuning = auto_entropy_tuning\n",
    "        self.soft_target_tau = soft_target_tau\n",
    "        self.discount = discount\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.qf1 = Soft_Q_Network(\n",
    "            input_size = 2*(state_dim + action_dim),\n",
    "            h1_size = layer_size,\n",
    "            h2_size = layer_size,\n",
    "            output_size=1\n",
    "\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.qf1_optimizer = optim.Adam(self.qf1.parameters(), lr=qf_lr)\n",
    "\n",
    "        self.qf2 = Soft_Q_Network(\n",
    "            input_size = 2 * (state_dim + action_dim),\n",
    "            h1_size = layer_size,\n",
    "            h2_size = layer_size,\n",
    "            output_size=1\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.qf2_optimizer = optim.Adam(self.qf2.parameters(), lr=qf_lr)\n",
    "\n",
    "        self.target_qf1 = Soft_Q_Network(\n",
    "            input_size = 2*(state_dim + action_dim),\n",
    "            h1_size = layer_size,\n",
    "            h2_size = layer_size,\n",
    "            output_size=1\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.target_qf2 = Soft_Q_Network(\n",
    "            input_size = 2*(state_dim + action_dim),\n",
    "            h1_size = layer_size,\n",
    "            h2_size = layer_size,\n",
    "            output_size=1\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.policy = Soft_Policy_Network(\n",
    "            input_size = state_dim,\n",
    "            h1_size = layer_size,\n",
    "            h2_size = layer_size,\n",
    "            output_mean_size = action_dim,\n",
    "            output_std_size = action_dim\n",
    "        ).to(self.device)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=policy_lr)\n",
    "\n",
    "        if self.auto_entropy_tuning:\n",
    "            self.target_entropy = -0.000\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            self.log_alpha_optim = optim.Adam([self.log_alpha], lr=a_lr)\n",
    "\n",
    "        # copy parameters of qf1/qf2 to target_qf1/target_qf2\n",
    "        for target_params, params in zip(self.target_qf1.parameters(), self.qf1.parameters()):\n",
    "            target_params.data.copy_(params)\n",
    "\n",
    "        for target_params, params in zip(self.target_qf2.parameters(), self.qf2.parameters()):\n",
    "            target_params.data.copy_(params)\n",
    "\n",
    "    def get_action(self, state,deterministic):\n",
    "        return self.policy.get_action(state,deterministic)\n",
    "\n",
    "    def save_weights(self):\n",
    "        torch.save(self.qf1.state_dict(), \"Models/checkpoint_masac_critic1_Agent\"+str(self.index)+\".pth\")\n",
    "        torch.save(self.qf2.state_dict(), \"Models/checkpoint_masac_critic2_Agent\"+str(self.index)+\".pth\")\n",
    "        torch.save(self.policy.state_dict(), \"Models/checkpoint_masac_actor_Agent\"+str(self.index)+\".pth\")\n",
    "\n",
    "    def load_weights(self,critic1_path,critic2_path,actor_path):\n",
    "        self.qf1.load_state_dict(torch.load(critic1_path))\n",
    "        self.qf1.eval()\n",
    "        print (\"Models: {} loaded...\".format(critic1_path))\n",
    "        self.qf2.load_state_dict(torch.load(critic2_path))\n",
    "        self.qf2.eval()\n",
    "        print (\"Models: {} loaded...\".format(critic2_path))\n",
    "        self.policy.load_state_dict(torch.load(actor_path))\n",
    "        self.policy.eval()\n",
    "        print (\"Models: {} loaded...\".format(actor_path))\n",
    "\n",
    "    def update(self,states, actions, rewards, next_states, dones,ma_agents):\n",
    "        num_agents = len(ma_agents)\n",
    "        #concatenate states, actions, next_states and next_actions\n",
    "        all_states = torch.cat(tuple(states[i] for i in range(num_agents)),dim=1)\n",
    "        all_actions = torch.cat(tuple(actions[i] for i in range(num_agents)),dim=1)\n",
    "        all_next_states = torch.cat(tuple(next_states[i] for i in range(num_agents)),dim=1)\n",
    "        local_rewards = rewards[self.index]\n",
    "        local_dones = dones[self.index]\n",
    "\n",
    "        ###### POLICY EVALUATION STEP ######\n",
    "\n",
    "        #Update the collective Q-function parameters for the agent\n",
    "        #Predict next actions after next_states for all the agents\n",
    "        next_actions=[]\n",
    "        next_log_pis = []\n",
    "\n",
    "        for agent in ma_agents:\n",
    "            local_next_actions, local_next_log_pis = agent.policy.sample(next_states[agent.index])\n",
    "            next_actions.append(local_next_actions)\n",
    "            next_log_pis.append (local_next_log_pis)\n",
    "\n",
    "        all_next_actions = torch.cat(tuple(next_actions[i] for i in range(num_agents)),dim=1)\n",
    "\n",
    "        next_qf1 = self.target_qf1.forward(all_next_states,all_next_actions)\n",
    "        next_qf2 = self.target_qf2.forward(all_next_states,all_next_actions)\n",
    "        next_q_target = torch.min(next_qf1,next_qf2) - self.alpha * next_log_pis[self.index]\n",
    "        expected_q = local_rewards + (1 - local_dones) * self.discount * next_q_target\n",
    "        curr_qf1 = self.qf1.forward(all_states,all_actions)\n",
    "        curr_qf2 = self.qf2.forward(all_states,all_actions)\n",
    "        qf1_loss = F.mse_loss(curr_qf1, expected_q.detach())\n",
    "        qf2_loss = F.mse_loss(curr_qf2, expected_q.detach())\n",
    "\n",
    "        #Update critic1 weights\n",
    "        self.qf1_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.qf1_optimizer.step()\n",
    "\n",
    "        #Update critic2 weights\n",
    "        self.qf2_optimizer.zero_grad()\n",
    "        qf2_loss.backward()\n",
    "        self.qf2_optimizer.step()\n",
    "\n",
    "        ###### POLICY IMPROVEMENT STEP ######\n",
    "        #Predict new actions after the current state\n",
    "        new_actions = []\n",
    "        new_log_pis = []\n",
    "\n",
    "        for agent in ma_agents:\n",
    "            local_new_actions, local_new_log_pis = agent.policy.sample(states[agent.index])\n",
    "            new_actions.append(local_new_actions)\n",
    "            new_log_pis.append (local_new_log_pis)\n",
    "\n",
    "        all_new_actions = torch.cat(tuple(new_actions[i] for i in range(num_agents)),dim=1)\n",
    "        local_log_pis = new_log_pis[self.index]\n",
    "\n",
    "        min_q = torch.min(self.qf1.forward(all_states, all_new_actions),\n",
    "                          self.qf2.forward(all_states, all_new_actions))\n",
    "\n",
    "        policy_loss = (self.alpha * local_log_pis - min_q).mean()\n",
    "\n",
    "        #Update actor weights\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        #print (\"Policy of agents {} updated\".format(self.index))\n",
    "\n",
    "        #Update target network weights at every iteration\n",
    "        for target_params, params in zip(self.target_qf1.parameters(), self.qf1.parameters()):\n",
    "            target_params.data.copy_(self.soft_target_tau * params + (1 - self.soft_target_tau) * target_params)\n",
    "\n",
    "        for target_params, params in zip(self.target_qf2.parameters(), self.qf2.parameters()):\n",
    "            target_params.data.copy_(self.soft_target_tau * params + (1 - self.soft_target_tau) * target_params)\n",
    "\n",
    "        #Adjust entropy temperature\n",
    "        if self.auto_entropy_tuning:\n",
    "            self.log_alpha_optim.zero_grad()\n",
    "            alpha_loss = (self.log_alpha * (-local_log_pis - self.target_entropy).detach()).mean()\n",
    "            alpha_loss.backward()\n",
    "            self.log_alpha_optim.step()\n",
    "            self.alpha = self.log_alpha.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "\n",
    "    def __init__(self, name, solve_score,state_dim,action_dim,num_agents,num_steps_per_epoch=1000):\n",
    "        self.name = name\n",
    "        self.solve_score = solve_score\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_agents = num_agents\n",
    "        self.num_steps_per_epoch = num_steps_per_epoch\n",
    "        self.game_score = GameScore()\n",
    "\n",
    "    def compute_episod_score(self, scores):\n",
    "        return np.max(scores)\n",
    "\n",
    "    def test_for_ending(self):\n",
    "        test1 = (self.game_score.last_moving_average>=self.solve_score)\n",
    "        test2 = (len(self.game_score.all_scores) >=100)\n",
    "        end_test = test1 and test2\n",
    "        return end_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiple_Agents:\n",
    "\n",
    "    def __init__(self, game,replay_buffer_size=50000,batch_size=256,load_mode=False,\\\n",
    "                 save_mode=True, episods_before_update=10):\n",
    "\n",
    "        self.game = game\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.load_mode = load_mode\n",
    "        self.save_mode=save_mode\n",
    "        self.episods_before_update = episods_before_update\n",
    "\n",
    "        self.episod_index = 0\n",
    "        self.num_agents = self.game.num_agents\n",
    "\n",
    "        # Creation of the agents\n",
    "        self.agents = [SAC_Agent(index=i,state_dim = self.game.state_dim,action_dim = self.game.action_dim) for i in range(self.num_agents)]\n",
    "        # Creation of the replay buffer and game scores\n",
    "        self.replay_buffer = Centralized_ReplayBuffer(self.replay_buffer_size, self.batch_size, 1,self.num_agents)\n",
    "\n",
    "        # Loading of existing agents\n",
    "        if load_mode:\n",
    "            file_name = \"Models/checkpoint_masac_\"\n",
    "            for agent in self.agents:\n",
    "                critic1_path = file_name + \"critic1_Agent\" + str(agent.index)+\".pth\"\n",
    "                critic2_path = file_name + \"critic2_Agent\" + str(agent.index)+\".pth\"\n",
    "                actor_path = file_name + \"actor_Agent\" + str(agent.index)+\".pth\"\n",
    "                agent.load_weights(critic1_path, critic2_path,actor_path)\n",
    "\n",
    "    def get_action(self,states,deterministic):\n",
    "        actions = []\n",
    "        for state,agent in zip(states,self.agents):\n",
    "            action = agent.get_action(state,deterministic)\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def update_per_step(self):\n",
    "        self.episod_index+=1\n",
    "\n",
    "        if self.update_gateway():\n",
    "            # Sample a batch of experiences from the centralized buffer\n",
    "            states, actions, rewards, next_states, dones = self.replay_buffer.sample()\n",
    "\n",
    "            # Update critics and actors\n",
    "            for agent in self.agents:\n",
    "                agent.update(states, actions, rewards, next_states, dones,self.agents)\n",
    "\n",
    "    def update_gateway(self):\n",
    "        test1 = self.replay_buffer.buffer_len() >=self.batch_size\n",
    "        test2 = self.episod_index > self.episods_before_update\n",
    "        gateway = test1 and test2\n",
    "        return gateway\n",
    "\n",
    "    def save_weights(self):\n",
    "        if self.save_mode:\n",
    "            for agent in self.agents:\n",
    "                agent.save_weights()\n",
    "\n",
    "    def training(self,num_epochs):\n",
    "        # Launch the environment\n",
    "\n",
    "        env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "        # get the default brain\n",
    "        brain_name = env.brain_names[0]\n",
    "        brain = env.brains[brain_name]\n",
    "        actions=np.zeros(self.game.num_agents,)\n",
    "\n",
    "        for each_iteration in range(num_epochs):\n",
    "\n",
    "            # Initialization\n",
    "            actions=np.zeros(self.game.num_agents,)\n",
    "\n",
    "            env_info=env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "            states = env_info.vector_observations\n",
    "\n",
    "            scores = np.zeros(self.game.num_agents)\n",
    "\n",
    "            for each_environment_step in range(self.game.num_steps_per_epoch):\n",
    "                #interacts with the environment by sampling actions and collect next_states, rewards and status\n",
    "                actions = self.get_action(states,deterministic = False)\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "                #computes scores of all the agents\n",
    "                scores += rewards\n",
    "\n",
    "                #Store the transition in the replay buffer\n",
    "                self.replay_buffer.add(states,actions,rewards,next_states,dones)\n",
    "\n",
    "                #updates the critic and actor\n",
    "                self.update_per_step()\n",
    "                states = next_states\n",
    "\n",
    "                if np.any(dones) or each_environment_step == self.game.num_steps_per_epoch - 1:\n",
    "                    break\n",
    "\n",
    "            episod_score = self.game.compute_episod_score(scores)\n",
    "            self.game.game_score.update_scores(episod_score)\n",
    "            self.game.game_score.display_score()\n",
    "\n",
    "            if self.game.test_for_ending():\n",
    "                print(\"\\nGame {} solved in {:d} episodes!\".format(self.game.name, each_iteration))\n",
    "                self.save_weights()\n",
    "                env.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameScore:\n",
    "    \n",
    "    def __init__(self, window_len=100,frequency_display = 10,display_chart=True,clear_output=True):\n",
    "        self.window_len = window_len\n",
    "        self.frequency_display = frequency_display\n",
    "        self.display_chart = display_chart\n",
    "        self.clear_output = clear_output\n",
    "        \n",
    "        # Initialization\n",
    "        self.episod_index = 0\n",
    "        # list containing scores from each episode\n",
    "        self.all_scores = []                      \n",
    "        self.all_moving_averages=[]\n",
    "        # last [window_len] scores - useful for calculating a moving average\n",
    "        self.scores_window = deque(maxlen=window_len)  \n",
    "        self.last_score = 0\n",
    "        self.last_moving_average  = 0\n",
    "        \n",
    "    def update_scores(self,episod_score):\n",
    "\n",
    "        self.episod_index+=1\n",
    "        self.all_scores.append(episod_score)           \n",
    "        self.scores_window.append(episod_score)\n",
    "        self.all_moving_averages.append(np.mean(self.scores_window))\n",
    "        \n",
    "        # Update general statistics\n",
    "        self.last_score = self.all_scores[-1]\n",
    "        self.last_moving_average  = self.all_moving_averages[-1]\n",
    "        \n",
    "    def display_score(self):        \n",
    "\n",
    "        if self.episod_index % self.frequency_display == 0:\n",
    "            clear_output(wait=self.clear_output)\n",
    "            print (\"Last Score: {:.2f} - Moving Average over last {} episods: {:.2f}\".\\\n",
    "                   format(self.last_score, self.episod_index, self.last_moving_average))\n",
    "            if self.display_chart:\n",
    "                fig = plt.figure()\n",
    "                ax = fig.add_subplot(111)\n",
    "                plt.plot(np.arange(++len(self.all_scores)), self.all_scores)\n",
    "                plt.ylabel('Score')\n",
    "                plt.xlabel('Episode #')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation parameters\n",
    "\n",
    "# PARAMETERS FOR TRAINING / TEST MODE\n",
    "train_mode = True #test mode if False\n",
    "load_mode=True #set it to True in test mode (train_mode = False)\n",
    "save_mode = True #save the networks - train_mode only\n",
    "\n",
    "my_game = Game(name=\"Unity Tennis\", solve_score=1.5, state_dim = 24, action_dim=2,num_agents=2)\n",
    "ma = Multiple_Agents(game = my_game,replay_buffer_size=50000,batch_size=256,\\\n",
    "                     load_mode=load_mode,save_mode=save_mode, episods_before_update=10)\n",
    "\n",
    "\n",
    "\n",
    "if train_mode:\n",
    "        \n",
    "    #env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "    # get the default brain\n",
    "    #brain_name = env.brain_names[0]\n",
    "    #brain = env.brains[brain_name]\n",
    "    #env_info=env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    ma.training(num_epochs = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "\n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    t_step=0\n",
    "    rewards_history=[]\n",
    "    nb_episodes = 10\n",
    "    former_actions = np.zeros((2,2))\n",
    "    \n",
    "    for episodes in range (nb_episodes):\n",
    "\n",
    "        scores = np.zeros(my_game.num_agents) # initialize the score (for each agent) \n",
    "        t_step=0\n",
    "        while True:\n",
    "            \n",
    "            actions = ma.get_action(states,True)                # select an action (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += rewards                                 # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            t_step+=1\n",
    "            rewards_history.append(rewards)\n",
    "\n",
    "            if np.all(dones) or t_step>2000:                   # exit loop if episode finished\n",
    "                break\n",
    "        print (\"Nadal vs Federer: {}\".format(np.sum(rewards_history,axis=0)))\n",
    "        print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "    print ('Total score: {}'.format(np.sum(rewards_history)))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from game import Game\n",
    "from multiple_agents import Multiple_Agents\n",
    "\n",
    "my_game = Game(name=\"Unity Tennis\", solve_score=1.5, state_dim = 24, action_dim=2,\\\n",
    "               num_agents=2,num_steps_per_epoch = 1000)\n",
    "\n",
    "ma = Multiple_Agents(game = my_game,replay_buffer_size=50000,batch_size=256,\\\n",
    "                     load_mode=True,save_mode=True, episods_before_update=10)\n",
    "\n",
    "ma.training(num_epochs = 3000)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
